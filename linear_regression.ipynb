{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "linear-regression.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNU8zWVA07dY7K84wdfNkel",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fix27/Colab-Store/blob/main/linear_regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xi0EjJMJQaRT"
      },
      "source": [
        "#!git clone --branch r1.13.0 --depth 1 https://github.com/tensorflow/models\n",
        "#!tf_upgrade_v2 \\\n",
        "#  --infile c.py \\\n",
        "#  --outfile c_regression_v2.py"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KCKueKPma5Ad"
      },
      "source": [
        "Вот как это можно сделать. В коде, приведенном ниже, мы будет приближать\n",
        "линейной регрессией функцию вида f = kx + b для k = 2 и b = 1; k и b будут\n",
        "параметрами, которые мы хотим обучить"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PkHBdrhMRTlV",
        "outputId": "719e911b-51dc-43bc-842b-1aac72d6cf29",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import numpy as np,tensorflow as tf\n",
        "n_samples, batch_size, num_steps = 1000, 100, 20000\n",
        "X_data = np.random.uniform(1, 10, (n_samples, 1))\n",
        "y_data = 2 * X_data + 1 + np.random.normal(0, 2, (n_samples, 1))\n",
        "X = tf.Variable(tf.ones(shape=(batch_size, 1)), dtype=tf.float32)\n",
        "y = tf.Variable(tf.ones(shape=(batch_size, 1)), dtype=tf.float32)\n",
        "#X = tf.compat.v1.placeholder(tf.float32, shape=(batch_size, 1))\n",
        "#y = tf.compat.v1.placeholder(tf.float32, shape=(batch_size, 1))\n",
        "with tf.compat.v1.variable_scope('linear-regression'):\n",
        "  k = tf.Variable(tf.random.normal((1, 1)), name='slope')\n",
        "  b = tf.Variable(tf.zeros((1,)), name='bias')\n",
        "y_pred = tf.matmul(X, k) + b\n",
        "loss = tf.reduce_sum(input_tensor=(y - y_pred) ** 2)\n",
        "opt = tf.keras.optimizers.SGD()\n",
        "#tf.compat.v1.disable_eager_execution()\n",
        "optimizer = tf.compat.v1.train.GradientDescentOptimizer(learning_rate=0.0000001).minimize(loss)\n",
        "display_step = 100\n",
        "with tf.compat.v1.Session() as sess:\n",
        "  sess.run(tf.compat.v1.initialize_all_variables())\n",
        "  for i in range(num_steps):\n",
        "    indices = np.random.choice(n_samples, batch_size)\n",
        "    X_batch, y_batch = X_data[indices], y_data[indices]\n",
        "    _, loss_val, k_val, b_val = sess.run([ optimizer, loss, k, b ],\n",
        "     feed_dict = { X : X_batch, y : y_batch })\n",
        "    if (i+1) % display_step == 0:\n",
        "      print('Эпоха %d: %.8f, k=%.4f, b=%.4f' %\n",
        "        (i+1, loss_val, k_val, b_val))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/tf_should_use.py:247: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
            "Instructions for updating:\n",
            "Use `tf.global_variables_initializer` instead.\n",
            "Эпоха 100: 325.34658813, k=-0.8074, b=0.0036\n",
            "Эпоха 200: 322.73284912, k=-0.8038, b=0.0072\n",
            "Эпоха 300: 320.14007568, k=-0.8002, b=0.0108\n",
            "Эпоха 400: 317.56823730, k=-0.7966, b=0.0143\n",
            "Эпоха 500: 315.01696777, k=-0.7931, b=0.0179\n",
            "Эпоха 600: 312.48638916, k=-0.7895, b=0.0214\n",
            "Эпоха 700: 309.97604370, k=-0.7860, b=0.0250\n",
            "Эпоха 800: 307.48593140, k=-0.7825, b=0.0285\n",
            "Эпоха 900: 305.01586914, k=-0.7790, b=0.0320\n",
            "Эпоха 1000: 302.56555176, k=-0.7755, b=0.0355\n",
            "Эпоха 1100: 300.13565063, k=-0.7720, b=0.0389\n",
            "Эпоха 1200: 297.72625732, k=-0.7686, b=0.0424\n",
            "Эпоха 1300: 295.33615112, k=-0.7651, b=0.0458\n",
            "Эпоха 1400: 292.96539307, k=-0.7617, b=0.0493\n",
            "Эпоха 1500: 290.61361694, k=-0.7583, b=0.0527\n",
            "Эпоха 1600: 288.28057861, k=-0.7549, b=0.0561\n",
            "Эпоха 1700: 285.96646118, k=-0.7515, b=0.0595\n",
            "Эпоха 1800: 283.67086792, k=-0.7481, b=0.0629\n",
            "Эпоха 1900: 281.39364624, k=-0.7448, b=0.0662\n",
            "Эпоха 2000: 279.13488770, k=-0.7414, b=0.0696\n",
            "Эпоха 2100: 276.89416504, k=-0.7381, b=0.0729\n",
            "Эпоха 2200: 274.67138672, k=-0.7348, b=0.0762\n",
            "Эпоха 2300: 272.46643066, k=-0.7315, b=0.0795\n",
            "Эпоха 2400: 270.27935791, k=-0.7282, b=0.0828\n",
            "Эпоха 2500: 268.11029053, k=-0.7249, b=0.0861\n",
            "Эпоха 2600: 265.95996094, k=-0.7216, b=0.0894\n",
            "Эпоха 2700: 263.82693481, k=-0.7184, b=0.0926\n",
            "Эпоха 2800: 261.71102905, k=-0.7151, b=0.0959\n",
            "Эпоха 2900: 259.61224365, k=-0.7119, b=0.0991\n",
            "Эпоха 3000: 257.53018188, k=-0.7087, b=0.1023\n",
            "Эпоха 3100: 255.46485901, k=-0.7055, b=0.1055\n",
            "Эпоха 3200: 253.41607666, k=-0.7023, b=0.1087\n",
            "Эпоха 3300: 251.38375854, k=-0.6991, b=0.1119\n",
            "Эпоха 3400: 249.36773682, k=-0.6960, b=0.1150\n",
            "Эпоха 3500: 247.36793518, k=-0.6928, b=0.1182\n",
            "Эпоха 3600: 245.38421631, k=-0.6897, b=0.1213\n",
            "Эпоха 3700: 243.41638184, k=-0.6865, b=0.1245\n",
            "Эпоха 3800: 241.46430969, k=-0.6834, b=0.1276\n",
            "Эпоха 3900: 239.52787781, k=-0.6803, b=0.1307\n",
            "Эпоха 4000: 237.60699463, k=-0.6773, b=0.1338\n",
            "Эпоха 4100: 235.70268250, k=-0.6742, b=0.1368\n",
            "Эпоха 4200: 233.81365967, k=-0.6711, b=0.1399\n",
            "Эпоха 4300: 231.93986511, k=-0.6681, b=0.1430\n",
            "Эпоха 4400: 230.08108521, k=-0.6650, b=0.1460\n",
            "Эпоха 4500: 228.23722839, k=-0.6620, b=0.1490\n",
            "Эпоха 4600: 226.40808105, k=-0.6590, b=0.1520\n",
            "Эпоха 4700: 224.59364319, k=-0.6560, b=0.1551\n",
            "Эпоха 4800: 222.79367065, k=-0.6530, b=0.1580\n",
            "Эпоха 4900: 221.00819397, k=-0.6500, b=0.1610\n",
            "Эпоха 5000: 219.23703003, k=-0.6471, b=0.1640\n",
            "Эпоха 5100: 217.47998047, k=-0.6441, b=0.1669\n",
            "Эпоха 5200: 215.73707581, k=-0.6412, b=0.1699\n",
            "Эпоха 5300: 214.00811768, k=-0.6382, b=0.1728\n",
            "Эпоха 5400: 212.29295349, k=-0.6353, b=0.1757\n",
            "Эпоха 5500: 210.59159851, k=-0.6324, b=0.1786\n",
            "Эпоха 5600: 208.90386963, k=-0.6295, b=0.1815\n",
            "Эпоха 5700: 207.22962952, k=-0.6267, b=0.1844\n",
            "Эпоха 5800: 205.56880188, k=-0.6238, b=0.1873\n",
            "Эпоха 5900: 203.92132568, k=-0.6209, b=0.1902\n",
            "Эпоха 6000: 202.28706360, k=-0.6181, b=0.1930\n",
            "Эпоха 6100: 200.66586304, k=-0.6152, b=0.1959\n",
            "Эпоха 6200: 199.05769348, k=-0.6124, b=0.1987\n",
            "Эпоха 6300: 197.46238708, k=-0.6096, b=0.2015\n",
            "Эпоха 6400: 195.87982178, k=-0.6068, b=0.2043\n",
            "Эпоха 6500: 194.30998230, k=-0.6040, b=0.2071\n",
            "Эпоха 6600: 192.75267029, k=-0.6012, b=0.2099\n",
            "Эпоха 6700: 191.20780945, k=-0.5985, b=0.2127\n",
            "Эпоха 6800: 189.67536926, k=-0.5957, b=0.2154\n",
            "Эпоха 6900: 188.15519714, k=-0.5930, b=0.2182\n",
            "Эпоха 7000: 186.64724731, k=-0.5902, b=0.2209\n",
            "Эпоха 7100: 185.15139771, k=-0.5875, b=0.2236\n",
            "Эпоха 7200: 183.66748047, k=-0.5848, b=0.2263\n",
            "Эпоха 7300: 182.19544983, k=-0.5821, b=0.2290\n",
            "Эпоха 7400: 180.73518372, k=-0.5794, b=0.2317\n",
            "Эпоха 7500: 179.28724670, k=-0.5767, b=0.2344\n",
            "Эпоха 7600: 177.85195923, k=-0.5741, b=0.2371\n",
            "Эпоха 7700: 176.42814636, k=-0.5714, b=0.2398\n",
            "Эпоха 7800: 175.01570129, k=-0.5688, b=0.2424\n",
            "Эпоха 7900: 173.61451721, k=-0.5661, b=0.2450\n",
            "Эпоха 8000: 172.22531128, k=-0.5635, b=0.2477\n",
            "Эпоха 8100: 170.84747314, k=-0.5609, b=0.2503\n",
            "Эпоха 8200: 169.48054504, k=-0.5583, b=0.2529\n",
            "Эпоха 8300: 168.12449646, k=-0.5557, b=0.2555\n",
            "Эпоха 8400: 166.77929688, k=-0.5531, b=0.2581\n",
            "Эпоха 8500: 165.44497681, k=-0.5505, b=0.2607\n",
            "Эпоха 8600: 164.12126160, k=-0.5480, b=0.2632\n",
            "Эпоха 8700: 162.80805969, k=-0.5454, b=0.2658\n",
            "Эпоха 8800: 161.50543213, k=-0.5429, b=0.2683\n",
            "Эпоха 8900: 160.21325684, k=-0.5403, b=0.2709\n",
            "Эпоха 9000: 158.93133545, k=-0.5378, b=0.2734\n",
            "Эпоха 9100: 157.65959167, k=-0.5353, b=0.2759\n",
            "Эпоха 9200: 156.39822388, k=-0.5328, b=0.2784\n",
            "Эпоха 9300: 155.14682007, k=-0.5303, b=0.2809\n",
            "Эпоха 9400: 153.90531921, k=-0.5278, b=0.2834\n",
            "Эпоха 9500: 152.67395020, k=-0.5254, b=0.2859\n",
            "Эпоха 9600: 151.45233154, k=-0.5229, b=0.2884\n",
            "Эпоха 9700: 150.24035645, k=-0.5204, b=0.2908\n",
            "Эпоха 9800: 149.03829956, k=-0.5180, b=0.2933\n",
            "Эпоха 9900: 147.84573364, k=-0.5156, b=0.2957\n",
            "Эпоха 10000: 146.66265869, k=-0.5131, b=0.2981\n",
            "Эпоха 10100: 145.48916626, k=-0.5107, b=0.3005\n",
            "Эпоха 10200: 144.32498169, k=-0.5083, b=0.3029\n",
            "Эпоха 10300: 143.17010498, k=-0.5059, b=0.3053\n",
            "Эпоха 10400: 142.02452087, k=-0.5036, b=0.3077\n",
            "Эпоха 10500: 140.88790894, k=-0.5012, b=0.3101\n",
            "Эпоха 10600: 139.76060486, k=-0.4988, b=0.3125\n",
            "Эпоха 10700: 138.64219666, k=-0.4965, b=0.3148\n",
            "Эпоха 10800: 137.53277588, k=-0.4941, b=0.3172\n",
            "Эпоха 10900: 136.43223572, k=-0.4918, b=0.3195\n",
            "Эпоха 11000: 135.34045410, k=-0.4895, b=0.3219\n",
            "Эпоха 11100: 134.25741577, k=-0.4871, b=0.3242\n",
            "Эпоха 11200: 133.18299866, k=-0.4848, b=0.3265\n",
            "Эпоха 11300: 132.11721802, k=-0.4825, b=0.3288\n",
            "Эпоха 11400: 131.05996704, k=-0.4802, b=0.3311\n",
            "Эпоха 11500: 130.01112366, k=-0.4780, b=0.3334\n",
            "Эпоха 11600: 128.97067261, k=-0.4757, b=0.3357\n",
            "Эпоха 11700: 127.93856812, k=-0.4734, b=0.3379\n",
            "Эпоха 11800: 126.91470337, k=-0.4712, b=0.3402\n",
            "Эпоха 11900: 125.89904022, k=-0.4689, b=0.3424\n",
            "Эпоха 12000: 124.89146423, k=-0.4667, b=0.3447\n",
            "Эпоха 12100: 123.89194489, k=-0.4645, b=0.3469\n",
            "Эпоха 12200: 122.90042877, k=-0.4623, b=0.3491\n",
            "Эпоха 12300: 121.91684723, k=-0.4600, b=0.3513\n",
            "Эпоха 12400: 120.94114685, k=-0.4578, b=0.3535\n",
            "Эпоха 12500: 119.97318268, k=-0.4557, b=0.3557\n",
            "Эпоха 12600: 119.01300812, k=-0.4535, b=0.3579\n",
            "Эпоха 12700: 118.06049347, k=-0.4513, b=0.3601\n",
            "Эпоха 12800: 117.11559296, k=-0.4491, b=0.3623\n",
            "Эпоха 12900: 116.17823792, k=-0.4470, b=0.3644\n",
            "Эпоха 13000: 115.24839783, k=-0.4448, b=0.3666\n",
            "Эпоха 13100: 114.32598877, k=-0.4427, b=0.3687\n",
            "Эпоха 13200: 113.41094208, k=-0.4406, b=0.3709\n",
            "Эпоха 13300: 112.50323486, k=-0.4385, b=0.3730\n",
            "Эпоха 13400: 111.60277557, k=-0.4363, b=0.3751\n",
            "Эпоха 13500: 110.70946503, k=-0.4342, b=0.3772\n",
            "Эпоха 13600: 109.82331848, k=-0.4321, b=0.3793\n",
            "Эпоха 13700: 108.94427490, k=-0.4301, b=0.3814\n",
            "Эпоха 13800: 108.07360077, k=-0.4280, b=0.3835\n",
            "Эпоха 13900: 107.21029663, k=-0.4259, b=0.3856\n",
            "Эпоха 14000: 106.35385132, k=-0.4238, b=0.3876\n",
            "Эпоха 14100: 105.50426483, k=-0.4218, b=0.3897\n",
            "Эпоха 14200: 104.66146851, k=-0.4197, b=0.3917\n",
            "Эпоха 14300: 103.82539368, k=-0.4177, b=0.3938\n",
            "Эпоха 14400: 102.99595642, k=-0.4157, b=0.3958\n",
            "Эпоха 14500: 102.17315674, k=-0.4137, b=0.3978\n",
            "Эпоха 14600: 101.35693359, k=-0.4116, b=0.3999\n",
            "Эпоха 14700: 100.54721069, k=-0.4096, b=0.4019\n",
            "Эпоха 14800: 99.74392700, k=-0.4076, b=0.4039\n",
            "Эпоха 14900: 98.94706726, k=-0.4057, b=0.4059\n",
            "Эпоха 15000: 98.15658569, k=-0.4037, b=0.4078\n",
            "Эпоха 15100: 97.37237549, k=-0.4017, b=0.4098\n",
            "Эпоха 15200: 96.59443665, k=-0.3997, b=0.4118\n",
            "Эпоха 15300: 95.82271576, k=-0.3978, b=0.4137\n",
            "Эпоха 15400: 95.05712128, k=-0.3958, b=0.4157\n",
            "Эпоха 15500: 94.29767609, k=-0.3939, b=0.4176\n",
            "Эпоха 15600: 93.54425049, k=-0.3920, b=0.4196\n",
            "Эпоха 15700: 92.79684448, k=-0.3900, b=0.4215\n",
            "Эпоха 15800: 92.05543518, k=-0.3881, b=0.4234\n",
            "Эпоха 15900: 91.31990051, k=-0.3862, b=0.4254\n",
            "Эпоха 16000: 90.59027863, k=-0.3843, b=0.4273\n",
            "Эпоха 16100: 89.86643219, k=-0.3824, b=0.4292\n",
            "Эпоха 16200: 89.14837646, k=-0.3805, b=0.4311\n",
            "Эпоха 16300: 88.43604279, k=-0.3786, b=0.4329\n",
            "Эпоха 16400: 87.72940063, k=-0.3768, b=0.4348\n",
            "Эпоха 16500: 87.02838898, k=-0.3749, b=0.4367\n",
            "Эпоха 16600: 86.33299255, k=-0.3730, b=0.4385\n",
            "Эпоха 16700: 85.64311981, k=-0.3712, b=0.4404\n",
            "Эпоха 16800: 84.95875549, k=-0.3694, b=0.4423\n",
            "Эпоха 16900: 84.27986145, k=-0.3675, b=0.4441\n",
            "Эпоха 17000: 83.60638428, k=-0.3657, b=0.4459\n",
            "Эпоха 17100: 82.93827057, k=-0.3639, b=0.4477\n",
            "Эпоха 17200: 82.27549744, k=-0.3621, b=0.4496\n",
            "Эпоха 17300: 81.61800385, k=-0.3603, b=0.4514\n",
            "Эпоха 17400: 80.96575165, k=-0.3585, b=0.4532\n",
            "Эпоха 17500: 80.31870270, k=-0.3567, b=0.4550\n",
            "Эпоха 17600: 79.67682648, k=-0.3549, b=0.4568\n",
            "Эпоха 17700: 79.04006958, k=-0.3531, b=0.4585\n",
            "Эпоха 17800: 78.40837860, k=-0.3513, b=0.4603\n",
            "Эпоха 17900: 77.78175354, k=-0.3496, b=0.4621\n",
            "Эпоха 18000: 77.16007996, k=-0.3478, b=0.4638\n",
            "Эпоха 18100: 76.54341888, k=-0.3461, b=0.4656\n",
            "Эпоха 18200: 75.93165588, k=-0.3443, b=0.4673\n",
            "Эпоха 18300: 75.32476807, k=-0.3426, b=0.4691\n",
            "Эпоха 18400: 74.72271729, k=-0.3409, b=0.4708\n",
            "Эпоха 18500: 74.12549591, k=-0.3391, b=0.4725\n",
            "Эпоха 18600: 73.53301239, k=-0.3374, b=0.4743\n",
            "Эпоха 18700: 72.94527435, k=-0.3357, b=0.4760\n",
            "Эпоха 18800: 72.36224365, k=-0.3340, b=0.4777\n",
            "Эпоха 18900: 71.78382874, k=-0.3323, b=0.4794\n",
            "Эпоха 19000: 71.21004486, k=-0.3306, b=0.4811\n",
            "Эпоха 19100: 70.64081573, k=-0.3290, b=0.4828\n",
            "Эпоха 19200: 70.07617188, k=-0.3273, b=0.4844\n",
            "Эпоха 19300: 69.51599884, k=-0.3256, b=0.4861\n",
            "Эпоха 19400: 68.96031952, k=-0.3240, b=0.4878\n",
            "Эпоха 19500: 68.40904999, k=-0.3223, b=0.4894\n",
            "Эпоха 19600: 67.86219788, k=-0.3207, b=0.4911\n",
            "Эпоха 19700: 67.31969452, k=-0.3190, b=0.4927\n",
            "Эпоха 19800: 66.78153229, k=-0.3174, b=0.4944\n",
            "Эпоха 19900: 66.24765778, k=-0.3158, b=0.4960\n",
            "Эпоха 20000: 65.71802521, k=-0.3141, b=0.4976\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pz3srgIubYa0",
        "outputId": "5c7d0ade-3187-48ea-f048-3b6a98f101d8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import numpy as np\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Input, Dense, Activation\n",
        "logr = Sequential()\n",
        "logr.add(Dense(1, input_dim=2, activation='sigmoid'))\n",
        "logr.compile(loss='binary_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
        "def sampler(n, x, y):\n",
        "  return np.random.normal(size=[n, 2]) + [x, y]\n",
        "def sample_data(n=1000, p0=(-1., -1.), p1=(1., 1.)):\n",
        "  zeros, ones = np.zeros((n, 1)), np.ones((n, 1))\n",
        "  labels = np.vstack([zeros, ones])\n",
        "  z_sample = sampler(n, x=p0[0], y=p0[1])\n",
        "  o_sample = sampler(n, x=p1[0], y=p1[1])\n",
        "  return np.vstack([z_sample, o_sample]), labels\n",
        "X_train, Y_train = sample_data()\n",
        "X_test, Y_test = sample_data(100)\n",
        "logr.fit(X_train, Y_train, batch_size=16, nb_epoch=100,\n",
        "verbose=1, validation_data=(X_test, Y_test))\n"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
            "Train on 2000 samples, validate on 200 samples\n",
            "Epoch 1/100\n",
            "1856/2000 [==========================>...] - ETA: 0s - loss: 0.3533 - accuracy: 0.8782WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training_v1.py:2048: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
            "2000/2000 [==============================] - 0s 71us/sample - loss: 0.3484 - accuracy: 0.8820 - val_loss: 0.3050 - val_accuracy: 0.8900\n",
            "Epoch 2/100\n",
            "2000/2000 [==============================] - 0s 53us/sample - loss: 0.2929 - accuracy: 0.9075 - val_loss: 0.2731 - val_accuracy: 0.9000\n",
            "Epoch 3/100\n",
            "2000/2000 [==============================] - 0s 57us/sample - loss: 0.2643 - accuracy: 0.9160 - val_loss: 0.2554 - val_accuracy: 0.9100\n",
            "Epoch 4/100\n",
            "2000/2000 [==============================] - 0s 57us/sample - loss: 0.2471 - accuracy: 0.9150 - val_loss: 0.2443 - val_accuracy: 0.9200\n",
            "Epoch 5/100\n",
            "2000/2000 [==============================] - 0s 59us/sample - loss: 0.2358 - accuracy: 0.9155 - val_loss: 0.2368 - val_accuracy: 0.9250\n",
            "Epoch 6/100\n",
            "2000/2000 [==============================] - 0s 57us/sample - loss: 0.2279 - accuracy: 0.9165 - val_loss: 0.2315 - val_accuracy: 0.9300\n",
            "Epoch 7/100\n",
            "2000/2000 [==============================] - 0s 57us/sample - loss: 0.2221 - accuracy: 0.9185 - val_loss: 0.2276 - val_accuracy: 0.9300\n",
            "Epoch 8/100\n",
            "2000/2000 [==============================] - 0s 52us/sample - loss: 0.2177 - accuracy: 0.9200 - val_loss: 0.2246 - val_accuracy: 0.9300\n",
            "Epoch 9/100\n",
            "2000/2000 [==============================] - 0s 61us/sample - loss: 0.2143 - accuracy: 0.9205 - val_loss: 0.2223 - val_accuracy: 0.9300\n",
            "Epoch 10/100\n",
            "2000/2000 [==============================] - 0s 59us/sample - loss: 0.2116 - accuracy: 0.9205 - val_loss: 0.2204 - val_accuracy: 0.9300\n",
            "Epoch 11/100\n",
            "2000/2000 [==============================] - 0s 59us/sample - loss: 0.2094 - accuracy: 0.9205 - val_loss: 0.2189 - val_accuracy: 0.9300\n",
            "Epoch 12/100\n",
            "2000/2000 [==============================] - 0s 56us/sample - loss: 0.2075 - accuracy: 0.9210 - val_loss: 0.2177 - val_accuracy: 0.9300\n",
            "Epoch 13/100\n",
            "2000/2000 [==============================] - 0s 57us/sample - loss: 0.2060 - accuracy: 0.9220 - val_loss: 0.2167 - val_accuracy: 0.9350\n",
            "Epoch 14/100\n",
            "2000/2000 [==============================] - 0s 57us/sample - loss: 0.2048 - accuracy: 0.9215 - val_loss: 0.2158 - val_accuracy: 0.9350\n",
            "Epoch 15/100\n",
            "2000/2000 [==============================] - 0s 52us/sample - loss: 0.2037 - accuracy: 0.9225 - val_loss: 0.2151 - val_accuracy: 0.9350\n",
            "Epoch 16/100\n",
            "2000/2000 [==============================] - 0s 51us/sample - loss: 0.2028 - accuracy: 0.9225 - val_loss: 0.2144 - val_accuracy: 0.9350\n",
            "Epoch 17/100\n",
            "2000/2000 [==============================] - 0s 61us/sample - loss: 0.2020 - accuracy: 0.9225 - val_loss: 0.2139 - val_accuracy: 0.9350\n",
            "Epoch 18/100\n",
            "2000/2000 [==============================] - 0s 54us/sample - loss: 0.2013 - accuracy: 0.9235 - val_loss: 0.2134 - val_accuracy: 0.9350\n",
            "Epoch 19/100\n",
            "2000/2000 [==============================] - 0s 53us/sample - loss: 0.2007 - accuracy: 0.9235 - val_loss: 0.2130 - val_accuracy: 0.9400\n",
            "Epoch 20/100\n",
            "2000/2000 [==============================] - 0s 62us/sample - loss: 0.2001 - accuracy: 0.9235 - val_loss: 0.2127 - val_accuracy: 0.9400\n",
            "Epoch 21/100\n",
            "2000/2000 [==============================] - 0s 58us/sample - loss: 0.1997 - accuracy: 0.9235 - val_loss: 0.2123 - val_accuracy: 0.9400\n",
            "Epoch 22/100\n",
            "2000/2000 [==============================] - 0s 62us/sample - loss: 0.1993 - accuracy: 0.9230 - val_loss: 0.2121 - val_accuracy: 0.9400\n",
            "Epoch 23/100\n",
            "2000/2000 [==============================] - 0s 58us/sample - loss: 0.1989 - accuracy: 0.9230 - val_loss: 0.2118 - val_accuracy: 0.9400\n",
            "Epoch 24/100\n",
            "2000/2000 [==============================] - 0s 53us/sample - loss: 0.1986 - accuracy: 0.9230 - val_loss: 0.2116 - val_accuracy: 0.9400\n",
            "Epoch 25/100\n",
            "2000/2000 [==============================] - 0s 61us/sample - loss: 0.1983 - accuracy: 0.9230 - val_loss: 0.2114 - val_accuracy: 0.9400\n",
            "Epoch 26/100\n",
            "2000/2000 [==============================] - 0s 59us/sample - loss: 0.1980 - accuracy: 0.9235 - val_loss: 0.2112 - val_accuracy: 0.9400\n",
            "Epoch 27/100\n",
            "2000/2000 [==============================] - 0s 57us/sample - loss: 0.1978 - accuracy: 0.9225 - val_loss: 0.2111 - val_accuracy: 0.9400\n",
            "Epoch 28/100\n",
            "2000/2000 [==============================] - 0s 54us/sample - loss: 0.1976 - accuracy: 0.9230 - val_loss: 0.2109 - val_accuracy: 0.9400\n",
            "Epoch 29/100\n",
            "2000/2000 [==============================] - 0s 54us/sample - loss: 0.1974 - accuracy: 0.9230 - val_loss: 0.2108 - val_accuracy: 0.9400\n",
            "Epoch 30/100\n",
            "2000/2000 [==============================] - 0s 61us/sample - loss: 0.1972 - accuracy: 0.9225 - val_loss: 0.2107 - val_accuracy: 0.9400\n",
            "Epoch 31/100\n",
            "2000/2000 [==============================] - 0s 58us/sample - loss: 0.1970 - accuracy: 0.9225 - val_loss: 0.2106 - val_accuracy: 0.9400\n",
            "Epoch 32/100\n",
            "2000/2000 [==============================] - 0s 57us/sample - loss: 0.1969 - accuracy: 0.9225 - val_loss: 0.2105 - val_accuracy: 0.9400\n",
            "Epoch 33/100\n",
            "2000/2000 [==============================] - 0s 60us/sample - loss: 0.1968 - accuracy: 0.9230 - val_loss: 0.2104 - val_accuracy: 0.9400\n",
            "Epoch 34/100\n",
            "2000/2000 [==============================] - 0s 63us/sample - loss: 0.1966 - accuracy: 0.9230 - val_loss: 0.2103 - val_accuracy: 0.9400\n",
            "Epoch 35/100\n",
            "2000/2000 [==============================] - 0s 60us/sample - loss: 0.1965 - accuracy: 0.9225 - val_loss: 0.2102 - val_accuracy: 0.9400\n",
            "Epoch 36/100\n",
            "2000/2000 [==============================] - 0s 57us/sample - loss: 0.1964 - accuracy: 0.9230 - val_loss: 0.2101 - val_accuracy: 0.9400\n",
            "Epoch 37/100\n",
            "2000/2000 [==============================] - 0s 60us/sample - loss: 0.1963 - accuracy: 0.9225 - val_loss: 0.2101 - val_accuracy: 0.9400\n",
            "Epoch 38/100\n",
            "2000/2000 [==============================] - 0s 59us/sample - loss: 0.1962 - accuracy: 0.9225 - val_loss: 0.2100 - val_accuracy: 0.9400\n",
            "Epoch 39/100\n",
            "2000/2000 [==============================] - 0s 59us/sample - loss: 0.1962 - accuracy: 0.9230 - val_loss: 0.2100 - val_accuracy: 0.9400\n",
            "Epoch 40/100\n",
            "2000/2000 [==============================] - 0s 61us/sample - loss: 0.1961 - accuracy: 0.9225 - val_loss: 0.2099 - val_accuracy: 0.9400\n",
            "Epoch 41/100\n",
            "2000/2000 [==============================] - 0s 54us/sample - loss: 0.1960 - accuracy: 0.9230 - val_loss: 0.2099 - val_accuracy: 0.9400\n",
            "Epoch 42/100\n",
            "2000/2000 [==============================] - 0s 62us/sample - loss: 0.1960 - accuracy: 0.9230 - val_loss: 0.2098 - val_accuracy: 0.9400\n",
            "Epoch 43/100\n",
            "2000/2000 [==============================] - 0s 62us/sample - loss: 0.1959 - accuracy: 0.9230 - val_loss: 0.2098 - val_accuracy: 0.9400\n",
            "Epoch 44/100\n",
            "2000/2000 [==============================] - 0s 54us/sample - loss: 0.1959 - accuracy: 0.9230 - val_loss: 0.2098 - val_accuracy: 0.9400\n",
            "Epoch 45/100\n",
            "2000/2000 [==============================] - 0s 60us/sample - loss: 0.1958 - accuracy: 0.9230 - val_loss: 0.2098 - val_accuracy: 0.9400\n",
            "Epoch 46/100\n",
            "2000/2000 [==============================] - 0s 53us/sample - loss: 0.1958 - accuracy: 0.9230 - val_loss: 0.2097 - val_accuracy: 0.9400\n",
            "Epoch 47/100\n",
            "2000/2000 [==============================] - 0s 53us/sample - loss: 0.1957 - accuracy: 0.9225 - val_loss: 0.2097 - val_accuracy: 0.9400\n",
            "Epoch 48/100\n",
            "2000/2000 [==============================] - 0s 58us/sample - loss: 0.1957 - accuracy: 0.9230 - val_loss: 0.2097 - val_accuracy: 0.9400\n",
            "Epoch 49/100\n",
            "2000/2000 [==============================] - 0s 53us/sample - loss: 0.1957 - accuracy: 0.9230 - val_loss: 0.2097 - val_accuracy: 0.9400\n",
            "Epoch 50/100\n",
            "2000/2000 [==============================] - 0s 57us/sample - loss: 0.1956 - accuracy: 0.9230 - val_loss: 0.2096 - val_accuracy: 0.9400\n",
            "Epoch 51/100\n",
            "2000/2000 [==============================] - 0s 66us/sample - loss: 0.1956 - accuracy: 0.9230 - val_loss: 0.2096 - val_accuracy: 0.9400\n",
            "Epoch 52/100\n",
            "2000/2000 [==============================] - 0s 57us/sample - loss: 0.1956 - accuracy: 0.9230 - val_loss: 0.2096 - val_accuracy: 0.9400\n",
            "Epoch 53/100\n",
            "2000/2000 [==============================] - 0s 59us/sample - loss: 0.1956 - accuracy: 0.9230 - val_loss: 0.2096 - val_accuracy: 0.9400\n",
            "Epoch 54/100\n",
            "2000/2000 [==============================] - 0s 58us/sample - loss: 0.1955 - accuracy: 0.9230 - val_loss: 0.2096 - val_accuracy: 0.9400\n",
            "Epoch 55/100\n",
            "2000/2000 [==============================] - 0s 54us/sample - loss: 0.1955 - accuracy: 0.9230 - val_loss: 0.2095 - val_accuracy: 0.9400\n",
            "Epoch 56/100\n",
            "2000/2000 [==============================] - 0s 59us/sample - loss: 0.1955 - accuracy: 0.9235 - val_loss: 0.2095 - val_accuracy: 0.9400\n",
            "Epoch 57/100\n",
            "2000/2000 [==============================] - 0s 60us/sample - loss: 0.1955 - accuracy: 0.9230 - val_loss: 0.2095 - val_accuracy: 0.9400\n",
            "Epoch 58/100\n",
            "2000/2000 [==============================] - 0s 59us/sample - loss: 0.1955 - accuracy: 0.9230 - val_loss: 0.2095 - val_accuracy: 0.9400\n",
            "Epoch 59/100\n",
            "2000/2000 [==============================] - 0s 69us/sample - loss: 0.1954 - accuracy: 0.9235 - val_loss: 0.2095 - val_accuracy: 0.9400\n",
            "Epoch 60/100\n",
            "2000/2000 [==============================] - 0s 60us/sample - loss: 0.1954 - accuracy: 0.9230 - val_loss: 0.2095 - val_accuracy: 0.9400\n",
            "Epoch 61/100\n",
            "2000/2000 [==============================] - 0s 59us/sample - loss: 0.1954 - accuracy: 0.9230 - val_loss: 0.2095 - val_accuracy: 0.9400\n",
            "Epoch 62/100\n",
            "2000/2000 [==============================] - 0s 61us/sample - loss: 0.1954 - accuracy: 0.9230 - val_loss: 0.2095 - val_accuracy: 0.9400\n",
            "Epoch 63/100\n",
            "2000/2000 [==============================] - 0s 60us/sample - loss: 0.1954 - accuracy: 0.9230 - val_loss: 0.2095 - val_accuracy: 0.9400\n",
            "Epoch 64/100\n",
            "2000/2000 [==============================] - 0s 64us/sample - loss: 0.1954 - accuracy: 0.9230 - val_loss: 0.2095 - val_accuracy: 0.9400\n",
            "Epoch 65/100\n",
            "2000/2000 [==============================] - 0s 64us/sample - loss: 0.1954 - accuracy: 0.9235 - val_loss: 0.2095 - val_accuracy: 0.9400\n",
            "Epoch 66/100\n",
            "2000/2000 [==============================] - 0s 58us/sample - loss: 0.1954 - accuracy: 0.9230 - val_loss: 0.2094 - val_accuracy: 0.9400\n",
            "Epoch 67/100\n",
            "2000/2000 [==============================] - 0s 58us/sample - loss: 0.1954 - accuracy: 0.9230 - val_loss: 0.2094 - val_accuracy: 0.9400\n",
            "Epoch 68/100\n",
            "2000/2000 [==============================] - 0s 54us/sample - loss: 0.1953 - accuracy: 0.9230 - val_loss: 0.2094 - val_accuracy: 0.9400\n",
            "Epoch 69/100\n",
            "2000/2000 [==============================] - 0s 58us/sample - loss: 0.1953 - accuracy: 0.9225 - val_loss: 0.2094 - val_accuracy: 0.9400\n",
            "Epoch 70/100\n",
            "2000/2000 [==============================] - 0s 67us/sample - loss: 0.1953 - accuracy: 0.9230 - val_loss: 0.2094 - val_accuracy: 0.9400\n",
            "Epoch 71/100\n",
            "2000/2000 [==============================] - 0s 62us/sample - loss: 0.1953 - accuracy: 0.9240 - val_loss: 0.2094 - val_accuracy: 0.9400\n",
            "Epoch 72/100\n",
            "2000/2000 [==============================] - 0s 56us/sample - loss: 0.1953 - accuracy: 0.9230 - val_loss: 0.2094 - val_accuracy: 0.9400\n",
            "Epoch 73/100\n",
            "2000/2000 [==============================] - 0s 57us/sample - loss: 0.1953 - accuracy: 0.9230 - val_loss: 0.2094 - val_accuracy: 0.9400\n",
            "Epoch 74/100\n",
            "2000/2000 [==============================] - 0s 59us/sample - loss: 0.1953 - accuracy: 0.9235 - val_loss: 0.2094 - val_accuracy: 0.9400\n",
            "Epoch 75/100\n",
            "2000/2000 [==============================] - 0s 70us/sample - loss: 0.1953 - accuracy: 0.9240 - val_loss: 0.2094 - val_accuracy: 0.9400\n",
            "Epoch 76/100\n",
            "2000/2000 [==============================] - 0s 57us/sample - loss: 0.1953 - accuracy: 0.9230 - val_loss: 0.2094 - val_accuracy: 0.9400\n",
            "Epoch 77/100\n",
            "2000/2000 [==============================] - 0s 59us/sample - loss: 0.1953 - accuracy: 0.9230 - val_loss: 0.2094 - val_accuracy: 0.9400\n",
            "Epoch 78/100\n",
            "2000/2000 [==============================] - 0s 62us/sample - loss: 0.1953 - accuracy: 0.9235 - val_loss: 0.2094 - val_accuracy: 0.9400\n",
            "Epoch 79/100\n",
            "2000/2000 [==============================] - 0s 60us/sample - loss: 0.1953 - accuracy: 0.9235 - val_loss: 0.2094 - val_accuracy: 0.9400\n",
            "Epoch 80/100\n",
            "2000/2000 [==============================] - 0s 60us/sample - loss: 0.1953 - accuracy: 0.9235 - val_loss: 0.2094 - val_accuracy: 0.9400\n",
            "Epoch 81/100\n",
            "2000/2000 [==============================] - 0s 58us/sample - loss: 0.1953 - accuracy: 0.9235 - val_loss: 0.2094 - val_accuracy: 0.9400\n",
            "Epoch 82/100\n",
            "2000/2000 [==============================] - 0s 68us/sample - loss: 0.1953 - accuracy: 0.9230 - val_loss: 0.2094 - val_accuracy: 0.9400\n",
            "Epoch 83/100\n",
            "2000/2000 [==============================] - 0s 61us/sample - loss: 0.1953 - accuracy: 0.9235 - val_loss: 0.2094 - val_accuracy: 0.9400\n",
            "Epoch 84/100\n",
            "2000/2000 [==============================] - 0s 63us/sample - loss: 0.1953 - accuracy: 0.9230 - val_loss: 0.2094 - val_accuracy: 0.9400\n",
            "Epoch 85/100\n",
            "2000/2000 [==============================] - 0s 58us/sample - loss: 0.1953 - accuracy: 0.9235 - val_loss: 0.2094 - val_accuracy: 0.9400\n",
            "Epoch 86/100\n",
            "2000/2000 [==============================] - 0s 61us/sample - loss: 0.1953 - accuracy: 0.9230 - val_loss: 0.2094 - val_accuracy: 0.9400\n",
            "Epoch 87/100\n",
            "2000/2000 [==============================] - 0s 59us/sample - loss: 0.1953 - accuracy: 0.9230 - val_loss: 0.2094 - val_accuracy: 0.9400\n",
            "Epoch 88/100\n",
            "2000/2000 [==============================] - 0s 65us/sample - loss: 0.1953 - accuracy: 0.9230 - val_loss: 0.2094 - val_accuracy: 0.9400\n",
            "Epoch 89/100\n",
            "2000/2000 [==============================] - 0s 65us/sample - loss: 0.1952 - accuracy: 0.9230 - val_loss: 0.2094 - val_accuracy: 0.9400\n",
            "Epoch 90/100\n",
            "2000/2000 [==============================] - 0s 61us/sample - loss: 0.1952 - accuracy: 0.9235 - val_loss: 0.2094 - val_accuracy: 0.9400\n",
            "Epoch 91/100\n",
            "2000/2000 [==============================] - 0s 70us/sample - loss: 0.1953 - accuracy: 0.9235 - val_loss: 0.2094 - val_accuracy: 0.9400\n",
            "Epoch 92/100\n",
            "2000/2000 [==============================] - 0s 67us/sample - loss: 0.1952 - accuracy: 0.9240 - val_loss: 0.2094 - val_accuracy: 0.9400\n",
            "Epoch 93/100\n",
            "2000/2000 [==============================] - 0s 61us/sample - loss: 0.1952 - accuracy: 0.9240 - val_loss: 0.2094 - val_accuracy: 0.9400\n",
            "Epoch 94/100\n",
            "2000/2000 [==============================] - 0s 67us/sample - loss: 0.1953 - accuracy: 0.9230 - val_loss: 0.2094 - val_accuracy: 0.9400\n",
            "Epoch 95/100\n",
            "2000/2000 [==============================] - 0s 59us/sample - loss: 0.1952 - accuracy: 0.9230 - val_loss: 0.2094 - val_accuracy: 0.9400\n",
            "Epoch 96/100\n",
            "2000/2000 [==============================] - 0s 55us/sample - loss: 0.1952 - accuracy: 0.9235 - val_loss: 0.2094 - val_accuracy: 0.9400\n",
            "Epoch 97/100\n",
            "2000/2000 [==============================] - 0s 61us/sample - loss: 0.1952 - accuracy: 0.9230 - val_loss: 0.2094 - val_accuracy: 0.9400\n",
            "Epoch 98/100\n",
            "2000/2000 [==============================] - 0s 62us/sample - loss: 0.1952 - accuracy: 0.9235 - val_loss: 0.2094 - val_accuracy: 0.9400\n",
            "Epoch 99/100\n",
            "2000/2000 [==============================] - 0s 61us/sample - loss: 0.1952 - accuracy: 0.9235 - val_loss: 0.2094 - val_accuracy: 0.9400\n",
            "Epoch 100/100\n",
            "2000/2000 [==============================] - 0s 62us/sample - loss: 0.1952 - accuracy: 0.9230 - val_loss: 0.2094 - val_accuracy: 0.9400\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f48fe3dc208>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    }
  ]
}